{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1: Implementando Regressão Linear do Zero\n",
    "\n",
    "Autora: Lília Sampaio\n",
    "\n",
    "### Código base \n",
    "\n",
    "Para iniciar as tarefas descritar para essa atividade, é necessário executar o código original obtido através do GitHub do autor que apresenta o vídeo \"How to do Linear Regression using Gradient Descent\", que é como segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learning_rate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_m = m_current - (learning_rate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    points = genfromtxt(\"data.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0\n",
    "    initial_m = 0\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando a função inicial do código acima, ```run()```, com os dados fornecidos também no GitHub do autor, obtemos o seguinte resultado para ```m``` e para ```b``` com $1000$ iterações e taxa de aprendizado de $0.0001$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "Running...\n",
      "After 1000 iterations b = 0.08893651993741346, m = 1.4777440851894448, error = 112.61481011613473\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Usando dados de escolaridade \n",
    "\n",
    "Agora usando os dados de escolaridade trabalhados em classe, o código muda para chamar ```school-data.csv``` e a função ```run``` fica da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_path):\n",
    "    points = genfromtxt(data_path, delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0\n",
    "    initial_m = 0\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a mesma configuração anterior de número de iterações e taxa de aprendizado, obtemos o resultado abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 1000 iterations b = -0.18234255376510086, m = 3.262182267596014, error = 103.39842291729676\n"
     ]
    }
   ],
   "source": [
    "run(\"income.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculando o valor do RSS a cada iteração\n",
    "\n",
    "Para calcular o valor do RSS, a função original ```compute_error_for_line_given_points()``` foi reescrita para retornar apenas o valor da soma do quadrado dos erros ao invés de sua média. Dessa forma, foi criada a função ```calculate_rss()```, como segue, e chamada no método ```gradient_descent_runner()``` após o fim de cada iteração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rss(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations, print_rss):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    rss_array = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "        squared_error = calculate_rss(b, m, points)\n",
    "        rss_array.append(squared_error)\n",
    "    \n",
    "    if print_rss:\n",
    "        print(\"RSS values for {0} iterations are {1}.\".format(num_iterations, rss_array))\n",
    "    return [b, m]\n",
    "\n",
    "def run(data_path, print_rss=False):\n",
    "    points = genfromtxt(data_path, delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0\n",
    "    initial_m = 0\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations, print_rss)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado se encontra na lista de valores abaixo contendo $1000$ itens, um para cada iteração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "RSS values for 1000 iterations are [79447.14379878416, 71435.20777869043, 64264.53040961913, 57846.77849791643, 52102.89394397851, 46962.119845960995, 42361.12886379743, 38243.24310606869, 34557.73592971116, 31259.207051614892, 28307.023274273037, 25664.817935926396, 23300.042919058902, 21183.567698552353, 19289.320490278456, 17593.96707953032, 16076.623372863956, 14718.598132350142, 13503.162723046531, 12415.345037268555, 11441.745057065482, 10570.369782863547, 9790.485494804614, 9092.485526826244, 8467.771924625973, 7908.649529685073, 7408.231184600101, 6960.352891971379, 6559.497881712548, 6200.728651386109, 5879.626142387334, 5592.235302703772, 5335.0163656519635, 5104.801244406793, 4898.754505158914, 4714.338438138678, 4549.28179622555, 4401.551816041983, 4269.329176866909, 4150.985588894107, 4045.0637347510246, 3950.2593171828303, 3865.4049917517095, 3789.4559866225372, 3721.477232288723, 3660.631842692717, 3606.1708058430027, 3557.4237569290235, 3513.790720270382, 3474.734718371543, 3439.775157034888, 3408.481905045042, 3380.4699954936254, 3355.394883471439, 3332.9482017087944, 3312.8539618788423, 3294.8651547687296, 3278.7607074369553, 3264.3427598729595, 3251.4342276108086, 3239.8766202714364, 3229.5280891606403, 3220.261679871688, 3211.963768366804, 3204.5326612720132, 3197.877343142775, 3191.9163552682376, 3186.5767922024424, 3181.7934036609845, 3177.5077907196255, 3173.6676864130427, 3170.2263118716055, 3167.1418000645626, 3164.3766800509134, 3161.8974153845866, 3159.6739909876146, 3157.6795434021597, 3155.890029866517, 3154.2839321385495, 3152.8419914180276, 3151.5469711024457, 3150.3834444537742, 3149.3376045604605, 3148.397094253656, 3147.5508538824565, 3146.788985072899, 3146.1026287924747, 3145.4838562179925, 3144.9255710624484, 3144.4214221577085, 3143.965725216106, 3143.553392807174, 3143.179871686907, 3142.841086707521, 3142.5333906167602, 3142.25351912835, 3141.9985507100805, 3141.765870594224, 3141.553138566882, 3141.3582601394914, 3141.179360747366, 3141.0147626574144, 3140.862964300586, 3140.722621774424, 3140.5925322878875, 3140.471619344462, 3140.3589194811075, 3140.2535703995727, 3140.154800343989, 3140.0619185938003, 3139.9743069549577, 3139.8914121445464, 3139.8127389750375, 3139.737844254202, 3139.666331325538, 3139.5978451819597, 3139.5320680925633, 3139.4687156885793, 3139.407533460306, 3139.348293621875, 3139.2907923052126, 3139.234847048634, 3139.180294549166, 3139.1269886508408, 3139.074798544246, 3139.023607155114, 3138.973309702123, 3138.92381240612, 3138.8750313348646, 3138.8268913691163, 3138.7793252772362, 3138.7322728869954, 3138.6856803443175, 3138.639499449869, 3138.593687065308, 3138.5482045818653, 3138.503017444739, 3138.4580947274244, 3138.413408750738, 3138.3689347418353, 3138.3246505290576, 3138.28053626879, 3138.236574201025, 3138.192748430576, 3138.1490447312626, 3138.1054503706778, 3138.061953953321, 3138.018545280241, 3137.975215223383, 3137.931955613152, 3137.8887591377616, 3137.845619253174, 3137.8025301024845, 3137.7594864437638, 3137.716483585496, 3137.6735173287716, 3137.6305839155775, 3137.5876799824873, 3137.5448025192445, 3137.501948831665, 3137.4591165084585, 3137.4163033915243, 3137.3735075493605, 3137.330727253272, 3137.2879609560787, 3137.245207273057, 3137.202464964877, 3137.1597329223387, 3137.117010152706, 3137.074295767485, 3137.0315889714666, 3136.9888890529414, 3136.946195374933, 3136.9035073673367, 3136.8608245199116, 3136.8181463759997, 3136.7754725268815, 3136.7328026067535, 3136.6901362882213, 3136.6474732782694, 3136.6048133146387, 3136.562156162615, 3136.5195016121233, 3136.476849475146, 3136.4341995834075, 3136.391551786297, 3136.3489059490184, 3136.306261950923, 3136.2636196840385, 3136.2209790517186, 3136.1783399674673, 3136.1357023538694, 3136.093066141642, 3136.050431268763, 3136.0077976797284, 3135.965165324863, 3135.9225341597007, 3135.879904144444, 3135.8372752434784, 3135.794647424918, 3135.752020660229, 3135.709394923874, 3135.6667701929873, 3135.6241464471104, 3135.5815236679314, 3135.538901839049, 3135.496280945798, 3135.4536609750385, 3135.411041915017, 3135.368423755207, 3135.3258064861907, 3135.283190099533, 3135.2405745876854, 3135.197959943888, 3135.1553461620915, 3135.112733236881, 3135.070121163403, 3135.0275099373207, 3134.984899554747, 3134.9422900121963, 3134.899681306555, 3134.8570734350265, 3134.8144663951184, 3134.771860184586, 3134.7292548014284, 3134.686650243845, 3134.644046510227, 3134.601443599133, 3134.5588415092716, 3134.5162402394835, 3134.473639788729, 3134.431040156082, 3134.3884413407013, 3134.345843341842, 3134.3032461588296, 3134.260649791067, 3134.2180542380083, 3134.175459499166, 3134.1328655741095, 3134.0902724624425, 3134.0476801638133, 3134.005088677901, 3133.9624980044264, 3133.9199081431275, 3133.877319093777, 3133.8347308561597, 3133.7921434300915, 3133.7495568154004, 3133.706971011932, 3133.664386019548, 3133.6218018381182, 3133.579218467529, 3133.5366359076784, 3133.494054158469, 3133.451473219818, 3133.4088930916373, 3133.366313773863, 3133.3237352664232, 3133.2811575692594, 3133.2385806823163, 3133.196004605538, 3133.1534293388804, 3133.1108548822954, 3133.068281235742, 3133.025708399182, 3132.9831363725807, 3132.940565155898, 3132.89799474911, 3132.8554251521764, 3132.8128563650785, 3132.7702883877796, 3132.7277212202607, 3132.6851548624936, 3132.642589314452, 3132.600024576118, 3132.5574606474643, 3132.5148975284737, 3132.47233521912, 3132.429773719388, 3132.3872130292525, 3132.3446531487, 3132.302094077705, 3132.259535816259, 3132.216978364331, 3132.1744217219143, 3132.131865888984, 3132.0893108655246, 3132.046756651518, 3132.004203246952, 3131.961650651805, 3131.91909886606, 3131.8765478897003, 3131.8339977227133, 3131.7914483650798, 3131.7488998167837, 3131.7063520778083, 3131.6638051481405, 3131.621259027762, 3131.5787137166567, 3131.536169214808, 3131.4936255222033, 3131.451082638824, 3131.4085405646542, 3131.3659992996804, 3131.323458843883, 3131.2809191972533, 3131.238380359767, 3131.1958423314154, 3131.15330511218, 3131.110768702045, 3131.068233100997, 3131.025698309017, 3130.983164326093, 3130.940631152206, 3130.898098787346, 3130.8555672314924, 3130.813036484631, 3130.770506546748, 3130.7279774178232, 3130.685449097848, 3130.6429215868025, 3130.6003948846733, 3130.5578689914414, 3130.515343907097, 3130.472819631621, 3130.4302961649987, 3130.387773507214, 3130.345251658254, 3130.3027306180998, 3130.260210386737, 3130.217690964153, 3130.175172350329, 3130.1326545452507, 3130.0901375489034, 3130.0476213612715, 3130.005105982339, 3129.962591412092, 3129.920077650511, 3129.877564697587, 3129.8350525532987, 3129.792541217634, 3129.7500306905768, 3129.7075209721124, 3129.665012062223, 3129.6225039608967, 3129.579996668114, 3129.5374901838622, 3129.494984508127, 3129.452479640892, 3129.409975582139, 3129.3674723318554, 3129.324969890027, 3129.282468256636, 3129.2399674316684, 3129.1974674151074, 3129.154968206938, 3129.1124698071467, 3129.069972215716, 3129.027475432632, 3128.9849794578777, 3128.9424842914386, 3128.899989933299, 3128.857496383446, 3128.8150036418583, 3128.7725117085283, 3128.7300205834326, 3128.687530266563, 3128.6450407578986, 3128.602552057427, 3128.5600641651336, 3128.5175770810015, 3128.475090805013, 3128.432605337159, 3128.390120677419, 3128.347636825778, 3128.3051537822203, 3128.262671546734, 3128.2201901193007, 3128.177709499906, 3128.135229688538, 3128.0927506851726, 3128.0502724898006, 3128.007795102406, 3127.9653185229754, 3127.9228427514886, 3127.880367787931, 3127.837893632291, 3127.7954202845526, 3127.752947744696, 3127.7104760127127, 3127.6680050885816, 3127.625534972288, 3127.583065663819, 3127.5405971631553, 3127.4981294702884, 3127.4556625851974, 3127.4131965078673, 3127.3707312382844, 3127.328266776434, 3127.285803122296, 3127.2433402758593, 3127.2008782371086, 3127.1584170060282, 3127.1159565826015, 3127.073496966814, 3127.0310381586482, 3126.9885801580945, 3126.9461229651297, 3126.9036665797453, 3126.8612110019208, 3126.818756231644, 3126.7763022689005, 3126.733849113668, 3126.6913967659393, 3126.648945225699, 3126.606494492925, 3126.5640445676067, 3126.521595449727, 3126.4791471392714, 3126.4366996362255, 3126.3942529405717, 3126.351807052296, 3126.3093619713823, 3126.266917697817, 3126.2244742315825, 3126.182031572667, 3126.1395897210505, 3126.0971486767166, 3126.054708439658, 3126.0122690098524, 3125.969830387287, 3125.927392571945, 3125.8849555638103, 3125.8425193628714, 3125.800083969111, 3125.7576493825113, 3125.715215603063, 3125.6727826307424, 3125.630350465542, 3125.5879191074414, 3125.5454885564277, 3125.503058812485, 3125.460629875597, 3125.4182017457492, 3125.3757744229256, 3125.3333479071125, 3125.2909221982927, 3125.248497296452, 3125.2060732015752, 3125.163649913644, 3125.1212274326476, 3125.0788057585687, 3125.0363848913885, 3124.993964831096, 3124.9515455776773, 3124.9091271311127, 3124.8667094913903, 3124.8242926584903, 3124.7818766324026, 3124.739461413106, 3124.6970470005917, 3124.6546333948418, 3124.6122205958377, 3124.5698086035677, 3124.527397418016, 3124.484987039164, 3124.442577467003, 3124.400168701514, 3124.3577607426787, 3124.315353590483, 3124.272947244917, 3124.230541705959, 3124.188136973596, 3124.145733047813, 3124.103329928596, 3124.0609276159257, 3124.0185261097904, 3123.9761254101736, 3123.9337255170562, 3123.891326430433, 3123.848928150277, 3123.806530676578, 3123.764134009321, 3123.7217381484916, 3123.6793430940734, 3123.6369488460505, 3123.594555404405, 3123.5521627691282, 3123.509770940199, 3123.4673799176053, 3123.42498970133, 3123.3826002913597, 3123.3402116876746, 3123.2978238902638, 3123.2554368991096, 3123.213050714199, 3123.1706653355154, 3123.128280763043, 3123.085896996766, 3123.043514036671, 3123.001131882739, 3122.9587505349605, 3122.9163699933133, 3122.873990257788, 3122.8316113283668, 3122.789233205035, 3122.7468558877754, 3122.7044793765763, 3122.6621036714164, 3122.619728772287, 3122.5773546791697, 3122.534981392049, 3122.4926089109085, 3122.450237235734, 3122.4078663665146, 3122.365496303224, 3122.323127045859, 3122.280758594398, 3122.238390948823, 3122.1960241091265, 3122.153658075287, 3122.1112928472903, 3122.0689284251243, 3122.02656480877, 3121.9842019982116, 3121.941839993439, 3121.899478794431, 3121.857118401175, 3121.8147588136553, 3121.7724000318563, 3121.7300420557617, 3121.6876848853594, 3121.6453285206335, 3121.6029729615643, 3121.560618208141, 3121.518264260346, 3121.4759111181666, 3121.4335587815826, 3121.391207250582, 3121.34885652515, 3121.306506605271, 3121.2641574909276, 3121.221809182109, 3121.1794616787924, 3121.13711498097, 3121.0947690886237, 3121.0524240017367, 3121.0100797202967, 3120.967736244283, 3120.9253935736842, 3120.883051708488, 3120.840710648672, 3120.7983703942273, 3120.7560309451364, 3120.7136923013813, 3120.6713544629483, 3120.629017429823, 3120.586681201991, 3120.5443457794345, 3120.5020111621384, 3120.4596773500903, 3120.417344343272, 3120.3750121416683, 3120.332680745267, 3120.2903501540486, 3120.248020368, 3120.2056913871065, 3120.16336321135, 3120.1210358407175, 3120.078709275192, 3120.036383514762, 3119.9940585594095, 3119.9517344091164, 3119.909411063872, 3119.8670885236616, 3119.824766788462, 3119.782445858269, 3119.740125733058, 3119.6978064128193, 3119.655487897534, 3119.6131701871886, 3119.5708532817707, 3119.5285371812574, 3119.486221885639, 3119.443907394904, 3119.4015937090267, 3119.3592808279986, 3119.316968751802, 3119.274657480424, 3119.2323470138504, 3119.190037352058, 3119.1477284950415, 3119.1054204427787, 3119.063113195258, 3119.0208067524604, 3118.978501114374, 3118.9361962809844, 3118.893892252273, 3118.8515890282247, 3118.8092866088264, 3118.7669849940603, 3118.7246841839146, 3118.682384178371, 3118.640084977414, 3118.5977865810314, 3118.555488989205, 3118.513192201918, 3118.470896219161, 3118.4286010409132, 3118.386306667163, 3118.34401309789, 3118.301720333085, 3118.2594283727285, 3118.217137216808, 3118.1748468653063, 3118.132557318207, 3118.0902685755004, 3118.0479806371636, 3118.0056935031857, 3117.963407173552, 3117.921121648243, 3117.8788369272484, 3117.8365530105507, 3117.7942698981337, 3117.7519875899843, 3117.709706086086, 3117.667425386422, 3117.625145490979, 3117.5828663997404, 3117.5405881126926, 3117.498310629821, 3117.456033951105, 3117.4137580765364, 3117.371483006095, 3117.3292087397667, 3117.2869352775356, 3117.244662619389, 3117.202390765308, 3117.160119715281, 3117.117849469292, 3117.075580027323, 3117.0333113893594, 3116.991043555386, 3116.9487765253925, 3116.906510299357, 3116.864244877266, 3116.8219802591034, 3116.779716444858, 3116.7374534345126, 3116.6951912280474, 3116.652929825455, 3116.610669226712, 3116.5684094318103, 3116.526150440732, 3116.4838922534614, 3116.441634869978, 3116.3993782902753, 3116.357122514334, 3116.3148675421407, 3116.2726133736746, 3116.2303600089263, 3116.1881074478806, 3116.145855690518, 3116.103604736825, 3116.0613545867864, 3116.0191052403866, 3115.9768566976127, 3115.934608958447, 3115.8923620228747, 3115.8501158908816, 3115.8078705624503, 3115.7656260375666, 3115.7233823162155, 3115.6811393983835, 3115.63889728405, 3115.5966559732037, 3115.5544154658296, 3115.512175761911, 3115.469936861433, 3115.427698764381, 3115.3854614707393, 3115.343224980492, 3115.300989293624, 3115.25875441012, 3115.2165203299655, 3115.174287053143, 3115.1320545796393, 3115.089822909442, 3115.0475920425297, 3115.0053619788905, 3114.9631327185075, 3114.920904261369, 3114.8786766074554, 3114.8364497567536, 3114.7942237092475, 3114.751998464924, 3114.709774023765, 3114.6675503857577, 3114.6253275508834, 3114.5831055191297, 3114.5408842904817, 3114.49866386492, 3114.456444242435, 3114.414225423008, 3114.372007406623, 3114.329790193268, 3114.2875737829254, 3114.24535817558, 3114.2031433712177, 3114.1609293698234, 3114.118716171378, 3114.076503775869, 3114.0342921832817, 3113.992081393602, 3113.949871406813, 3113.9076622228968, 3113.8654538418446, 3113.823246263634, 3113.7810394882554, 3113.7388335156897, 3113.6966283459246, 3113.654423978941, 3113.6122204147277, 3113.5700176532655, 3113.5278156945446, 3113.4856145385443, 3113.4434141852503, 3113.401214634651, 3113.3590158867282, 3113.316817941465, 3113.274620798848, 3113.232424458865, 3113.190228921495, 3113.1480341867305, 3113.1058402545455, 3113.0636471249327, 3113.0214547978767, 3112.979263273357, 3112.9370725513622, 3112.894882631878, 3112.8526935148866, 3112.810505200372, 3112.768317688324, 3112.7261309787204, 3112.683945071553, 3112.6417599668002, 3112.59957566445, 3112.5573921644886, 3112.5152094668956, 3112.4730275716615, 3112.4308464787696, 3112.3886661881984, 3112.3464866999416, 3112.3043080139814, 3112.262130130298, 3112.2199530488806, 3112.177776769714, 3112.135601292779, 3112.093426618064, 3112.0512527455544, 3112.0090796752343, 3111.9669074070816, 3111.924735941091, 3111.8825652772443, 3111.8403954155224, 3111.7982263559147, 3111.756058098402, 3111.713890642972, 3111.6717239896084, 3111.6295581382947, 3111.587393089019, 3111.545228841762, 3111.5030653965127, 3111.460902753252, 3111.418740911968, 3111.3765798726404, 3111.334419635259, 3111.2922601998066, 3111.250101566268, 3111.2079437346288, 3111.165786704872, 3111.123630476984, 3111.081475050948, 3111.0393204267507, 3110.9971666043757, 3110.955013583807, 3110.912861365029, 3110.870709948029, 3110.828559332792, 3110.7864095192986, 3110.744260507538, 3110.702112297492, 3110.6599648891456, 3110.617818282485, 3110.575672477495, 3110.5335274741565, 3110.4913832724615, 3110.4492398723883, 3110.4070972739255, 3110.364955477055, 3110.3228144817645, 3110.2806742880357, 3110.2385348958537, 3110.1963963052067, 3110.1542585160737, 3110.112121528447, 3110.0699853423052, 3110.0278499576348, 3109.9857153744215, 3109.9435815926486, 3109.9014486123033, 3109.859316433364, 3109.817185055826, 3109.775054479667, 3109.73292470487, 3109.6907957314243, 3109.648667559315, 3109.6065401885207, 3109.5644136190335, 3109.5222878508353, 3109.4801628839105, 3109.438038718241, 3109.3959153538176, 3109.3537927906204, 3109.3116710286376, 3109.2695500678506, 3109.2274299082433, 3109.1853105498067, 3109.14319199252, 3109.1010742363696, 3109.058957281341, 3109.0168411274176, 3108.974725774585, 3108.9326112228305, 3108.8904974721318, 3108.84838452248, 3108.8062723738585, 3108.7641610262485, 3108.72205047964, 3108.6799407340163, 3108.637831789358, 3108.595723645656, 3108.553616302894, 3108.511509761052, 3108.4694040201184, 3108.427299080077, 3108.3851949409145, 3108.343091602612, 3108.3009890651565, 3108.258887328534, 3108.2167863927284, 3108.1746862577224, 3108.1325869235025, 3108.0904883900525, 3108.0483906573577, 3108.006293725407, 3107.964197594178, 3107.9221022636607, 3107.8800077338346, 3107.8379140046895, 3107.795821076211, 3107.7537289483794, 3107.711637621182, 3107.6695470946006, 3107.6274573686237, 3107.585368443236, 3107.543280318421, 3107.5011929941625, 3107.4591064704455, 3107.417020747258, 3107.3749358245805, 3107.3328517023997, 3107.290768380699, 3107.2486858594652, 3107.206604138683, 3107.164523218336, 3107.122443098409, 3107.08036377889, 3107.03828525976, 3106.9962075410012, 3106.9541306226038, 3106.912054504554, 3106.8699791868294, 3106.827904669421, 3106.7858309523094, 3106.7437580354826, 3106.7016859189225, 3106.659614602617, 3106.617544086548, 3106.575474370702, 3106.533405455064, 3106.4913373396166, 3106.4492700243463, 3106.4072035092413, 3106.3651377942774, 3106.3230728794474, 3106.281008764735, 3106.23894545012, 3106.1968829355924, 3106.1548212211355, 3106.1127603067316, 3106.0707001923683, 3106.028640878032, 3105.986582363704, 3105.944524649369, 3105.9024677350135, 3105.860411620623, 3105.8183563061793, 3105.776301791673, 3105.734248077079, 3105.6921951623926, 3105.650143047593, 3105.6080917326635, 3105.5660412175926, 3105.523991502366, 3105.481942586964, 3105.4398944713744, 3105.3978471555793, 3105.35580063957, 3105.3137549233215, 3105.2717100068285, 3105.2296658900655, 3105.187622573027, 3105.1455800556955, 3105.1035383380513, 3105.0614974200807, 3105.01945730177, 3104.977417983106, 3104.9353794640706, 3104.893341744646, 3104.8513048248237, 3104.8092687045846, 3104.7672333839128, 3104.7251988627927, 3104.683165141212, 3104.6411322191548, 3104.5991000966033, 3104.5570687735462, 3104.5150382499633, 3104.4730085258434, 3104.4309796011694, 3104.38895147593, 3104.346924150105, 3104.304897623683, 3104.262871896644, 3104.220846968975, 3104.178822840661, 3104.13679951169, 3104.0947769820427, 3104.052755251706, 3104.010734320662, 3103.968714188899, 3103.9266948564014, 3103.8846763231504, 3103.842658589135, 3103.800641654335, 3103.7586255187407, 3103.716610182336, 3103.6745956451027, 3103.6325819070253, 3103.590568968094, 3103.548556828289, 3103.5065454875917, 3103.4645349459965, 3103.422525203484, 3103.3805162600356, 3103.338508115638, 3103.2965007702783, 3103.2544942239383, 3103.2124884766035, 3103.170483528261, 3103.128479378892, 3103.0864760284853, 3103.0444734770217, 3103.00247172449, 3102.9604707708722, 3102.9184706161514, 3102.8764712603174, 3102.8344727033523, 3102.7924749452404, 3102.7504779859655, 3102.708481825517, 3102.666486463874, 3102.6244919010273, 3102.582498136955, 3102.540505171648, 3102.4985130050854, 3102.456521637256, 3102.4145310681465, 3102.3725412977365, 3102.330552326015, 3102.2885641529638, 3102.246576778568, 3102.2045902028135, 3102.162604425686, 3102.1206194471692, 3102.078635267246, 3102.0366518859064, 3101.99466930313, 3101.9526875189026].\n",
      "After 1000 iterations b = -0.18234255376510086, m = 3.262182267596014, error = 103.39842291729676\n"
     ]
    }
   ],
   "source": [
    "run(\"income.csv\", print_rss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plotando Iterações X RSS\n",
    "\n",
    "Ao longo das iterações o RSS diminui. O gráfico abaixo mostra esse comportamento onde X é o número de iterações \n",
    "e Y o valor de RSS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/iterations_rss.png\" width=\"650\" hight=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testando valores de taxa de aprendizado e iterações\n",
    "\n",
    "Queremos obter os valores de **b = -39** e **m = 5**. Vamos começar modificando a função ```run()``` aumentado a taxa de aprendizado e o número de iterações em 10x (de $0.0001$ para $0.001$ e de $1000$ para $10000$). Vamos ainda reescrever a função ```gradient_descent_runner()``` para não mais imprimir o erro a cada iteração e dessa forma deixar o resultado final mais claro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 10000 iterations b = -24.13302001545096, m = 4.6879171746959, error = 41.01920667729307\n"
     ]
    }
   ],
   "source": [
    "def run(data_path, num_iterations, learning_rate, print_rss=False):\n",
    "    points = genfromtxt(data_path, delimiter=\",\")\n",
    "    initial_b = 0\n",
    "    initial_m = 0\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations, print_rss)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "\n",
    "run(\"income.csv\", num_iterations=10000, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos agora **b = -24.13** e **m = 4.68**. Vamos tunar o número de iterações mais um pouco, aumentando em 2x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 20000 iterations b = -33.530504059124134, m = 5.247330204302785, error = 31.49887307365348\n"
     ]
    }
   ],
   "source": [
    "run(\"income.csv\", num_iterations=20000, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos agora **b = -33.53** e **m = 5.24**. Vamos agora aumentar um pouco mais a taxa de aprendizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 20000 iterations b = -39.105305893911776, m = 5.579186770311661, error = 29.83436366491363\n"
     ]
    }
   ],
   "source": [
    "run(\"income.csv\", num_iterations=20000, learning_rate=0.0025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chegamos então a valores aproximados de **b = -39** e **m = 5** para uma taxa de aprendizado de $0.0025$ e $20000$ iterações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Definindo novo critério de parada\n",
    "\n",
    "Queremos agora modificar o algoritmo para considerar um critério de parada que é relacionado ao tamanho do gradiente. Aqui vamos estabelecer que, quando o passo do gradiente for menor que um determinado threshold, assumimos que convergimos. O código abaixo mostra uma modificação na função ```step_gradient()``` para retornar além dos novos ```b``` e ```m``` seus respectivos gradientes. Além disso, a função ```gradient_descent_runner()``` considera agora como critério de parada os gradientes de ```m``` e ```b``` serem maiores que o threshold pré-definido. Aqui chamaremos esse threshold de tolerância. Segue o código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current, points, learning_rate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_m = m_current - (learning_rate * m_gradient)\n",
    "    return [new_b, new_m, b_gradient, m_gradient]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, tolerance, print_rss):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    rss_array = []\n",
    "\n",
    "    num_iterations = 0\n",
    "    b_gradient = float(\"inf\")\n",
    "    m_gradient = float(\"inf\")\n",
    "    \n",
    "    while abs(b_gradient) > tolerance or abs(m_gradient) > tolerance:\n",
    "        b, m, b_gradient, m_gradient = step_gradient(b, m, array(points), learning_rate)\n",
    "        \n",
    "        squared_error = calculate_rss(b, m, points)\n",
    "        rss_array.append(squared_error)\n",
    "        \n",
    "        num_iterations +=1\n",
    "        \n",
    "        with open(\"graphs/iterations_gradient.csv\", \"a+\") as file:\n",
    "             file.write(\"{0},{1},{2}\\n\".format(num_iterations, b_gradient, m_gradient))\n",
    "    \n",
    "    if print_rss:\n",
    "        print(\"RSS values for {0} iterations are {1}.\".format(num_iterations, rss_array))\n",
    "    return [b, m, num_iterations]\n",
    "\n",
    "def run_gradient_descent(data_path, learning_rate, tolerance, print_rss=False):\n",
    "    points = genfromtxt(data_path, delimiter=\",\")\n",
    "    initial_b = 0\n",
    "    initial_m = 0\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m, num_iterations] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, tolerance, print_rss)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando o código com o novo critério de parada, taxa de aprendizado como a anterior $0.0025$ e tolerância $0.002$, obtemos o seguinte resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 31717 iterations b = -39.42523544598863, m = 5.5982315230514725, error = 29.828837286629224\n"
     ]
    }
   ],
   "source": [
    "run_gradient_descent(\"income.csv\", learning_rate=0.0025, tolerance=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os gráficos dos tamanhos de gradiente de m e b versus o número de iterações até a convergência é como segue:\n",
    "\n",
    "<img src=\"graphs/gradient_m.png\" width=\"650\" hight=\"450\" />\n",
    "<img src=\"graphs/gradient_b.png\" width=\"650\" hight=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Um valor de tolerância para b = -39 e m = 5\n",
    "\n",
    "Como executado anteriormente, com uma taxa de aprendizado $0.0025$ e tolerância $0.002$ já obtivemos **b = -39** e **m = 5**, o que é confirmado se executarmos novamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 31717 iterations b = -39.42523544598863, m = 5.5982315230514725, error = 29.828837286629224\n"
     ]
    }
   ],
   "source": [
    "run_gradient_descent(\"income.csv\", learning_rate=0.0025, tolerance=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Calculando coeficientes de regressão com equações normais\n",
    "\n",
    "Usando como base o pseudo-código visto em sala que calcula coeficientes de regressão usando equações normais, temos a seguinte solução: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(points):\n",
    "    tmp_x = 0\n",
    "    tmp_y = 0\n",
    "    n = len(points)\n",
    "\n",
    "    for i in range(n):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        tmp_x += x\n",
    "        tmp_y += y\n",
    "        \n",
    "    avg_x = tmp_x / n\n",
    "    avg_y = tmp_y / n\n",
    "    \n",
    "    a = 0\n",
    "    b = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        a += (x - avg_x) * (y - avg_y)\n",
    "        b += (x - avg_x) ** 2\n",
    "        \n",
    "    w1 = a / b\n",
    "    w0 = avg_y - (w1 * avg_x)\n",
    "    \n",
    "    print(\"Coeficientes da reta encontrados são m = {0} e b = {1}\".format(w0, w1))\n",
    "    \n",
    "def run_linear_regression(data_path):\n",
    "    points = genfromtxt(data_path, delimiter=\",\")\n",
    "    linear_regression(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando essa solução para o conjunto de dados ```\"school-data.csv\"```, temos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes da reta encontrados são m = -39.44625667909617 e b = 5.599482874119919\n"
     ]
    }
   ],
   "source": [
    "run_linear_regression(\"income.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora queremos comparar o tempo de processamento para a solução com gradiente descendente e a que usa equações normais. Para isso, reformulamos a função ```run()``` de cada uma das soluções para guardar seu tempo de execução. Temos então:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent(data_path, learning_rate, tolerance, print_rss=False):\n",
    "    points = genfromtxt(data_path, delimiter=\",\")\n",
    "    initial_b = 0\n",
    "    initial_m = 0\n",
    "    \n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    \n",
    "    import time;\n",
    "    initial_time = time.time()\n",
    "    [b, m, num_iterations] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, tolerance, print_rss)\n",
    "    final_time = time.time()\n",
    "\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "    print(\"Tempo de processamento para a solução com gradiente descendente: {0}s.\".format(final_time - initial_time))\n",
    "\n",
    "def run_linear_regression(data_path):\n",
    "    points = genfromtxt(data_path, delimiter=\",\")\n",
    "    \n",
    "    import time; \n",
    "    initial_time = time.time()\n",
    "    linear_regression(points)\n",
    "    final_time = time.time()\n",
    "    \n",
    "    print(\"Tempo de processamento para a solução com equações normais: {0}s.\".format(final_time - initial_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 31717 iterations b = -39.42523544598863, m = 5.5982315230514725, error = 29.828837286629224\n",
      "Tempo de processamento para a solução com gradiente descendente: 27.570305824279785s.\n"
     ]
    }
   ],
   "source": [
    "run_gradient_descent(\"income.csv\", learning_rate=0.0025, tolerance=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes da reta encontrados são m = -39.44625667909617 e b = 5.599482874119919\n",
      "Tempo de processamento para a solução com equações normais: 0.00017309188842773438s.\n"
     ]
    }
   ],
   "source": [
    "run_linear_regression(\"income.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o tempo de processamento é bem menor para a solução que usa equações normais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
